{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import pandas as pd\n",
    "import PyPDF2 \n",
    "import requests\n",
    "from io import BytesIO\n",
    "import re\n",
    "\n",
    "df = pd.DataFrame([[0 for i in range(0,19)]], columns=[\"Year\", \"Range\", \"Amount\", \"Dolnośląskie\", \"Kujawsko-Pomorski\", \"Lubelskie\", \"Lubuskie\", \"Łódzkie\", \"Małopolskie\", \"Mazowieckie\", \"Opolskie\", \"Podkarpackie\", \"Podlaskie\", \"Pomorskie\", \"Śląskie\", \"Świętokrzyskie\", \"Warmińsko-Mazurskie\", \"Wielkopolskie\", \"Zachodniopomorskie\"])\n",
    "            \n",
    "for i in range(2007,2020+1):\n",
    "    x = \"http://wwwold.pzh.gov.pl/oldpage/epimeld/grypa/\"+str(i)+\"/\"+str(i)+\".htm\"\n",
    "\n",
    "    soup = BeautifulSoup(x, 'html.parser')\n",
    "\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request('GET', x)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if a['href'].startswith(\"G\"):\n",
    "            x = \"http://wwwold.pzh.gov.pl/oldpage/epimeld/grypa/\"+str(i)+\"/\"+a['href']\n",
    "\n",
    "            r = requests.get(x)\n",
    "            f = io.BytesIO(r.content)\n",
    "            reader = PdfFileReader(f)\n",
    "            contents = reader.getPage(0).extractText().split('\\n')\n",
    "            contents = (', '.join(map(str, contents)))\n",
    "#             print(contents)\n",
    "\n",
    "            try:\n",
    "                start = re.search(r\"POLSKA , \",contents).end()\n",
    "                nr = contents[start:start+7]\n",
    "                nr = nr.split(' ')\n",
    "                \n",
    "                start = re.search(r\"Dolnoskie , \",contents).end()\n",
    "                dolnoslaskie = contents[start:start+7]\n",
    "                dolnoslaskie = dolnoslaskie.split(' ')\n",
    "                \n",
    "                start = re.search(r\"Kujawsko-Pomorskie , \",contents).end()\n",
    "                kujawskie = contents[start:start+7]\n",
    "                kujawskie = kujawskie.split(' ')\n",
    "                \n",
    "                start = re.search(r\"Lubelskie , \",contents).end()\n",
    "                lubel = contents[start:start+7]\n",
    "                lubel = lubel.split(' ')\n",
    "                \n",
    "                start = re.search(r\" Lubuskie , \",contents).end()\n",
    "                lubuskie = contents[start:start+7]\n",
    "                lubuskie = lubuskie.split(' ')\n",
    "                \n",
    "                start = re.search(r\" Łódzkie , \",contents).end()\n",
    "                lodzkie = contents[start:start+7]\n",
    "                lodzkie = lodzkie.split(' ')\n",
    "\n",
    "                start = re.search(r\" Małopolskie , \",contents).end()\n",
    "                malo = contents[start:start+7]\n",
    "                malo = malo.split(' ')\n",
    "\n",
    "                start = re.search(r\"Mazowieckie , \",contents).end()\n",
    "                mazo = contents[start:start+7]\n",
    "                mazo = mazo.split(' ')\n",
    "                \n",
    "                start = re.search(r\"Opolskie , \",contents).end()\n",
    "                opol = contents[start:start+7]\n",
    "                opol = opol.split(' ')\n",
    "            \n",
    "                start = re.search(r\"Podkarpackie , \",contents).end()\n",
    "                podk = contents[start:start+7]\n",
    "                podk = podk.split(' ')\n",
    "                \n",
    "                start = re.search(r\"Podlaskie , \",contents).end()\n",
    "                podla = contents[start:start+7]\n",
    "                podla = podla.split(' ')\n",
    "                \n",
    "                start = re.search(r\"Pomorskie , \",contents).end()\n",
    "                pom = contents[start:start+7]\n",
    "                pom = pom.split(' ')\n",
    "                \n",
    "                start = re.search(r\" ie , \",contents).end()\n",
    "                slaskie = contents[start:start+7]\n",
    "                slaskie = slaskie.split(' ')\n",
    "                \n",
    "                start = re.search(r\"witokrzyskie , \",contents).end()\n",
    "                swieto = contents[start:start+7]\n",
    "                swieto = swieto.split(' ')\n",
    "                \n",
    "                start = re.search(r\" Warmisko-Mazurskie , \",contents).end()\n",
    "                warmi = contents[start:start+7]\n",
    "                warmi = warmi.split(' ')\n",
    "                \n",
    "                start = re.search(r\" Wielkopolskie , \",contents).end()\n",
    "                wielko = contents[start:start+7]\n",
    "                wielko = wielko.split(' ')\n",
    "\n",
    "                start = re.search(r\"Zachodniopomorskie \",contents).end()\n",
    "                zacho = contents[start:start+7]\n",
    "                zacho = zacho.split(' ')\n",
    "\n",
    "                data = i\n",
    "                df_new = pd.DataFrame([[str(i), a['href'][5:8], nr[0], dolnoslaskie[0],\\\n",
    "                    kujawskie[0], lubel[0], lubuskie[0], lodzkie[0], malo[0], mazo[0],\\\n",
    "                    opol[0], podk[0], podla[0], pom[0], slaskie[0], swieto[0], warmi[0], wielko[0], zacho[0]]],\\\n",
    "                    columns=[\"Year\", \"Range\", \"Amount\", \"Dolnośląskie\", \"Kujawsko-Pomorski\", \"Lubelskie\", \"Lubuskie\", \"Łódzkie\", \"Małopolskie\", \"Mazowieckie\", \"Opolskie\", \"Podkarpackie\", \"Podlaskie\", \"Pomorskie\", \"Śląskie\", \"Świętokrzyskie\", \"Warmińsko-Mazurskie\", \"Wielkopolskie\", \"Zachodniopomorskie\"])\n",
    "                df = df.append(df_new)\n",
    "            except:\n",
    "                pass\n",
    "           \n",
    "\n",
    "df.to_csv(\"2007-2020.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Meldunki 2007-2020.csv\")\n",
    "df = df[1:]\n",
    "\n",
    "df[\"Date\"] = [0 for i in range(len(df))]\n",
    "Date_column = []\n",
    "\n",
    "#Add correct type of date\n",
    "for i in range(len(df)):\n",
    "    m = df.Range[i+1][0:2]\n",
    "    d = df.Range[i+1][2:3]\n",
    "    if d == \"A\":\n",
    "        d = \"01\"\n",
    "    elif d == \"B\":\n",
    "        d = \"08\"\n",
    "    elif d == \"C\":\n",
    "        d = \"16\"\n",
    "    elif d == \"D\":\n",
    "        d = \"23\"\n",
    "    Date = str(df.Year[i+1])+m+d\n",
    "    Date_column.append(Date)\n",
    "    \n",
    "df.Date = Date_column \n",
    "df.Date = pd.to_datetime(df.Date, format='%Y%m%d', errors='ignore') \n",
    "cols = df.columns.tolist()\n",
    "df=df[cols[-1:]+cols[:-1]]\n",
    "df.to_csv('Meldunki 2007-2020 poprawione.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([[0 for i in range(0,19)]], columns=[\"Year\", \"Range\", \"Amount\", \"Dolnośląskie\", \"Kujawsko-Pomorski\", \"Lubelskie\", \"Lubuskie\", \"Łódzkie\", \"Małopolskie\", \"Mazowieckie\", \"Opolskie\", \"Podkarpackie\", \"Podlaskie\", \"Pomorskie\", \"Śląskie\", \"Świętokrzyskie\", \"Warmińsko-Mazurskie\", \"Wielkopolskie\", \"Zachodniopomorskie\"])\n",
    " \n",
    "#Second range - different schema\n",
    "for i in range(2000,2006):\n",
    "    x = \"http://wwwold.pzh.gov.pl/oldpage/epimeld/grypa/\"+str(i)+\"/\"+str(i)+\".htm\"\n",
    "\n",
    "    soup = BeautifulSoup(x, 'html.parser')\n",
    "\n",
    "    http = urllib3.PoolManager()\n",
    "    response = http.request('GET', x)\n",
    "    soup = BeautifulSoup(response.data)\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        if a['href'].startswith(\"G\"):\n",
    "            x = \"http://wwwold.pzh.gov.pl/oldpage/epimeld/grypa/\"+str(i)+\"/\"+a['href']\n",
    "            #Download\n",
    "#Open downloaded and extract \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "  \n",
    "pdfFileObj = open('G_20_01B.pdf', 'rb') \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "  \n",
    "print(pdfReader.numPages) \n",
    "pageObj = pdfReader.getPage(0) \n",
    "  \n",
    "# print(pageObj.extractText()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
